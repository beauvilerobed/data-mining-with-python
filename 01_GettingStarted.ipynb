{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Data Mining\n",
    "\n",
    "Data mining is a methodology that we can employ to train computers to make decisions with data and forms the backbone of many high-tech systems of today.\n",
    "\n",
    "The Python language is fast growing in popularity, for a good reason. It gives the programmer a lot of flexibility; it has a large number of modules to perform different tasks; and Python code is usually more readable and concise than in any other languages. There is a large and an active community of researchers, practitioners, and beginners using Python for data mining.\n",
    "\n",
    "- **Introducing data mining**\n",
    "- **A simple affiniy analysis example**\n",
    "- **A simple classification example**\n",
    "- **What is classification**\n",
    "\n",
    "## Introducing data mining\n",
    "\n",
    "Data mining provides a way for a computer to learn how to make decisions with data. This decision could be predicting tomorrow's weather, blocking a spam email from entering your inbox, detecting the language of a website, or finding a new romance on a dating site. \n",
    "\n",
    "Data mining is part of algorithms, statistics, engineering, optimization, and computer science. We also use concepts and knowledge from other fields such as linguistics, neuroscience, or town planning. Applying it effectively usually requires this domain-specific knowledge to be integrated with the algorithms.\n",
    "\n",
    "We start our data mining process by creating a dataset, describing an aspect of the real world. Datasets comprise of two aspects:\n",
    "\n",
    "- Samples that are objects in the real world. This can be a book, photograph, animal, person, or any other object.\n",
    "- Features that are descriptions of the samples in our dataset. They could be the length, frequency of a given word, number of legs, date it was created, and so on. \n",
    "\n",
    "The next step is tuning the data mining algorithm. Each data mining algorithm has parameters, either within the algorithm or supplied by the user. This tuning allows the algorithm to learn how to make decisions about the data.\n",
    "\n",
    "## A simple affinity analysis example\n",
    "\n",
    "A common use case for data mining is to improve sales by asking a customer who is buying a product if he/she would like another similar product as well. This can be done through affinity analysis, which is the study of when things exist together.\n",
    "\n",
    "## What is affinity analysis?\n",
    "\n",
    "Affinity analysis is a type of data mining that gives similarity between samples (objects). This could be the similarity between the following:\n",
    "\n",
    "- users on a website, in order to provide varied services or targeted advertising\n",
    "- items to sell to those users, in order to provide recommended movies or products\n",
    "- human genes, in order to find people that share the same ancestors\n",
    "\n",
    "We can measure affinity in a number of ways. For instance, we can record how frequently two products are purchased together. We can also record the accuracy of the statement when a person buys object 1 and also when they buy object 2. \n",
    "\n",
    "## Product recommendations\n",
    "\n",
    "One of the issues with moving a traditional business online, such as commerce, is that tasks that used to be done by humans need to be automated in order for the online business to scale. One example of this is up-selling, or selling an extra item to a customer who is already buying. Automated product recommendations through data mining are one of the driving forces behind the e-commerce revolution that is turning billions of dollars per year into revenue.\n",
    "\n",
    "Product recommendations are based on the following idea: when two items are historically purchased together, they are more likely to be purchased together in the future. This sort of thinking is behind many product recommendation services, in both online and offline businesses.\n",
    "\n",
    "A very simple algorithm for this type of product recommendation algorithm is to simply find any historical case where a user has brought an item and to recommend other items that the historical user brought. In practice, simple algorithms such as this can do well, at least better than choosing random items to recommend. However, they can be improved upon significantly, which is where data mining comes in.\n",
    "\n",
    "To simplify the coding, we will consider only two items at a time. As an example, people may buy bread and milk at the same time at the supermarket. In this early example, we wish to find simple rules of the form:\n",
    "\n",
    "*If a person buys product X, then they are likely to purchase product Y*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'data/'\n",
    "AFFINITY_DATASET = DATA + 'affinity_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 100 samples and 5 features\n"
     ]
    }
   ],
   "source": [
    "x = np.loadtxt(AFFINITY_DATASET)\n",
    "n_samples, n_features = x.shape\n",
    "print(f'This dataset has {n_samples} samples and {n_features} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these features contain binary values, stating only whether the items were purchased and not how many of them were purchased. A 1 indicates that \"at least 1\" item was bought of this type, while a 0 indicates that absolutely none of that item was purchased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple ranking of rules\n",
    "\n",
    "We wish to find rules of the type If a person buys product X, then they are likely to purchase product Y. We can quite easily create a list of all of the rules in our dataset by simply finding all occasions when two products were purchased together. However, we then need a way to determine good rules from bad ones. This will allow us to choose specific products to recommend.\n",
    "\n",
    "Rules of this type can be measured in many ways, of which we will focus on two: **support** and **confidence**.\n",
    "\n",
    "#### Support \n",
    "is the number of times that a rule occurs in a dataset, which is computed by simply counting the number of samples that the rule is valid for. \n",
    "\n",
    "**Note**: It can sometimes be normalized by dividing by the total number of times the premise of the rule is valid, but we will simply count the total for this implementation.\n",
    "\n",
    "#### Confidence \n",
    "measures how accurate they are when they can be used. It can be computed by determining the percentage of times the rule applies when the premise applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The names of the features, for your reference.\n",
    "features = [\"bread\", \"milk\", \"cheese\", \"apples\", \"bananas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our first example, we will compute the Support and Confidence of the rule \"If a person buys Apples, they also buy Bananas\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 people bought Apples\n"
     ]
    }
   ],
   "source": [
    "# First, how many rows contain our premise: that a person is buying apples\n",
    "num_apple_purchases = 0\n",
    "for sample in x:\n",
    "    if sample[3] == 1:  # This person bought Apples\n",
    "        num_apple_purchases += 1\n",
    "print(f\"{num_apple_purchases} people bought Apples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can check if bananas were bought in a transaction by seeing if the value for `sample[4]` is equal to 1 (and so on). We can now compute the number of times our rule exists in our dataset and, from that, the confidence and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 cases of the rule being valid were discovered\n",
      "15 cases of the rule being invalid were discovered\n"
     ]
    }
   ],
   "source": [
    "# How many of the cases that a person bought Apples involved the people purchasing Bananas too?\n",
    "# Record both cases where the rule is valid and is invalid.\n",
    "rule_valid = 0\n",
    "rule_invalid = 0\n",
    "for sample in x:\n",
    "    if sample[3] == 1:  # This person bought Apples\n",
    "        if sample[4] == 1:\n",
    "            # This person bought both Apples and Bananas\n",
    "            rule_valid += 1\n",
    "        else:\n",
    "            # This person bought Apples, but not Bananas\n",
    "            rule_invalid += 1\n",
    "print(f\"{rule_valid} cases of the rule being valid were discovered\")\n",
    "print(f\"{rule_invalid} cases of the rule being invalid were discovered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The support is 21 and the confidence is 0.583.\n",
      "As a percentage, that is 58.3%.\n"
     ]
    }
   ],
   "source": [
    "# Now we have all the information needed to compute Support and Confidence\n",
    "support = rule_valid  # The Support is the number of times the rule is discovered.\n",
    "confidence = rule_valid / num_apple_purchases\n",
    "print(f\"The support is {support} and the confidence is {confidence:.3f}.\")\n",
    "# Confidence can be thought of as a percentage using the following:\n",
    "print(f\"As a percentage, that is {100 * confidence:.1f}%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to compute the confidence and support for all possible rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# Now compute for all possible rules\n",
    "valid_rules = defaultdict(int)\n",
    "invalid_rules = defaultdict(int)\n",
    "num_occurences = defaultdict(int)\n",
    "\n",
    "for sample in x:\n",
    "    for premise in range(n_features):\n",
    "        if sample[premise] == 0: \n",
    "            continue\n",
    "        # Record that the premise was bought in another transaction\n",
    "        num_occurences[premise] += 1\n",
    "        for conclusion in range(n_features):\n",
    "            if premise == conclusion:  # It makes little sense to measure if X -> X.\n",
    "                continue\n",
    "            if sample[conclusion] == 1:\n",
    "                # This person also bought the conclusion item\n",
    "                valid_rules[(premise, conclusion)] += 1\n",
    "            else:\n",
    "                # This person bought the premise, but not the conclusion\n",
    "                invalid_rules[(premise, conclusion)] += 1\n",
    "support = valid_rules\n",
    "confidence = defaultdict(float)\n",
    "for premise, conclusion in valid_rules.keys():\n",
    "    confidence[(premise, conclusion)] = valid_rules[(premise, conclusion)] / num_occurences[premise]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: If a person buys cheese they will also buy apples\n",
      " - Confidence: 0.610\n",
      " - Support: 25\n",
      "Rule: If a person buys cheese they will also buy bananas\n",
      " - Confidence: 0.659\n",
      " - Support: 27\n",
      "Rule: If a person buys apples they will also buy cheese\n",
      " - Confidence: 0.694\n",
      " - Support: 25\n",
      "Rule: If a person buys apples they will also buy bananas\n",
      " - Confidence: 0.583\n",
      " - Support: 21\n",
      "Rule: If a person buys bananas they will also buy cheese\n",
      " - Confidence: 0.458\n",
      " - Support: 27\n",
      "Rule: If a person buys bananas they will also buy apples\n",
      " - Confidence: 0.356\n",
      " - Support: 21\n",
      "Rule: If a person buys bread they will also buy milk\n",
      " - Confidence: 0.519\n",
      " - Support: 14\n",
      "Rule: If a person buys bread they will also buy apples\n",
      " - Confidence: 0.185\n",
      " - Support: 5\n",
      "Rule: If a person buys milk they will also buy bread\n",
      " - Confidence: 0.304\n",
      " - Support: 14\n",
      "Rule: If a person buys milk they will also buy apples\n",
      " - Confidence: 0.196\n",
      " - Support: 9\n",
      "Rule: If a person buys apples they will also buy bread\n",
      " - Confidence: 0.139\n",
      " - Support: 5\n",
      "Rule: If a person buys apples they will also buy milk\n",
      " - Confidence: 0.250\n",
      " - Support: 9\n",
      "Rule: If a person buys bread they will also buy cheese\n",
      " - Confidence: 0.148\n",
      " - Support: 4\n",
      "Rule: If a person buys cheese they will also buy bread\n",
      " - Confidence: 0.098\n",
      " - Support: 4\n",
      "Rule: If a person buys milk they will also buy bananas\n",
      " - Confidence: 0.413\n",
      " - Support: 19\n",
      "Rule: If a person buys bananas they will also buy milk\n",
      " - Confidence: 0.322\n",
      " - Support: 19\n",
      "Rule: If a person buys bread they will also buy bananas\n",
      " - Confidence: 0.630\n",
      " - Support: 17\n",
      "Rule: If a person buys bananas they will also buy bread\n",
      " - Confidence: 0.288\n",
      " - Support: 17\n",
      "Rule: If a person buys milk they will also buy cheese\n",
      " - Confidence: 0.152\n",
      " - Support: 7\n",
      "Rule: If a person buys cheese they will also buy milk\n",
      " - Confidence: 0.171\n",
      " - Support: 7\n"
     ]
    }
   ],
   "source": [
    "for premise, conclusion in confidence:\n",
    "    premise_name = features[premise]\n",
    "    conclusion_name = features[conclusion]\n",
    "    print(f\"Rule: If a person buys {premise_name} they will also buy {conclusion_name}\")\n",
    "    print(f\" - Confidence: {confidence[(premise, conclusion)]:.3f}\")\n",
    "    print(f\" - Support: {support[(premise, conclusion)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rule(premise, conclusion, support, confidence, features):\n",
    "    premise_name = features[premise]\n",
    "    conclusion_name = features[conclusion]\n",
    "    print(f\"Rule: If a person buys {premise_name} they will also buy {conclusion_name}\")\n",
    "    print(f\" - Confidence: {confidence[(premise, conclusion)]:.3f}\")\n",
    "    print(f\" - Support: {support[(premise, conclusion)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: If a person buys milk they will also buy apples\n",
      " - Confidence: 0.196\n",
      " - Support: 9\n"
     ]
    }
   ],
   "source": [
    "premise = 1\n",
    "conclusion = 3\n",
    "print_rule(premise, conclusion, support, confidence, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((2, 3), 25),\n",
      " ((2, 4), 27),\n",
      " ((3, 2), 25),\n",
      " ((3, 4), 21),\n",
      " ((4, 2), 27),\n",
      " ((4, 3), 21),\n",
      " ((0, 1), 14),\n",
      " ((0, 3), 5),\n",
      " ((1, 0), 14),\n",
      " ((1, 3), 9),\n",
      " ((3, 0), 5),\n",
      " ((3, 1), 9),\n",
      " ((0, 2), 4),\n",
      " ((2, 0), 4),\n",
      " ((1, 4), 19),\n",
      " ((4, 1), 19),\n",
      " ((0, 4), 17),\n",
      " ((4, 0), 17),\n",
      " ((1, 2), 7),\n",
      " ((2, 1), 7)]\n"
     ]
    }
   ],
   "source": [
    "# Sort by support\n",
    "from pprint import pprint\n",
    "pprint(list(support.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking to find the best rules\n",
    "\n",
    "Now that we can compute the support and confidence of all rules, we want to be able to find the best rules. To do this, we perform a ranking and print the ones with the highest values. We can do this for both the support and confidence values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_support = sorted(support.items(), key=itemgetter(1), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule #1\n",
      "Rule: If a person buys cheese they will also buy bananas\n",
      " - Confidence: 0.659\n",
      " - Support: 27\n",
      "Rule #2\n",
      "Rule: If a person buys bananas they will also buy cheese\n",
      " - Confidence: 0.458\n",
      " - Support: 27\n",
      "Rule #3\n",
      "Rule: If a person buys cheese they will also buy apples\n",
      " - Confidence: 0.610\n",
      " - Support: 25\n",
      "Rule #4\n",
      "Rule: If a person buys apples they will also buy cheese\n",
      " - Confidence: 0.694\n",
      " - Support: 25\n",
      "Rule #5\n",
      "Rule: If a person buys apples they will also buy bananas\n",
      " - Confidence: 0.583\n",
      " - Support: 21\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    print(f\"Rule #{index + 1}\")\n",
    "    (premise, conclusion) = sorted_support[index][0]\n",
    "    print_rule(premise, conclusion, support, confidence, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can print the top rules based on confidence. First, compute the sorted confidence list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_confidence = sorted(confidence.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule #1\n",
      "Rule: If a person buys apples they will also buy cheese\n",
      " - Confidence: 0.694\n",
      " - Support: 25\n",
      "Rule #2\n",
      "Rule: If a person buys cheese they will also buy bananas\n",
      " - Confidence: 0.659\n",
      " - Support: 27\n",
      "Rule #3\n",
      "Rule: If a person buys bread they will also buy bananas\n",
      " - Confidence: 0.630\n",
      " - Support: 17\n",
      "Rule #4\n",
      "Rule: If a person buys cheese they will also buy apples\n",
      " - Confidence: 0.610\n",
      " - Support: 25\n",
      "Rule #5\n",
      "Rule: If a person buys apples they will also buy bananas\n",
      " - Confidence: 0.583\n",
      " - Support: 21\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    print(f\"Rule #{index + 1}\")\n",
    "    (premise, conclusion) = sorted_confidence[index][0]\n",
    "    print_rule(premise, conclusion, support, confidence, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two rules are near the top of both lists. The first is **If a person buys apples, they will also buy cheese**, and the second is **If a person buys cheese, they will also buy bananas**. A store manager can use rules like these to organize their store. For example, if apples are on sale this week, put a display of cheeses nearby. Similarly, it would make little sense to put both bananas on sale at the same time as cheese, as nearly 66 percent of people buying cheese will buy bananas anyway—our sale won't increase banana purchases all that much.\n",
    "\n",
    "Data mining has great exploratory power in examples like this. A person can use data mining techniques to explore relationships within their datasets to find new insights.\n",
    "\n",
    "## A simple classification example\n",
    "\n",
    "In the affinity analysis example, we looked for correlations between different variables in our dataset. In classification, we instead have a single variable that we are interested in and that we call the class (also called the target). If, in the previous example, we were interested in how to make people buy more apples, we could set that variable to be the class and look for classification rules that obtain that goal. We would then look only for rules that relate to that goal.\n",
    "\n",
    "## What is classification?\n",
    "\n",
    "Classification is one of the largest uses of data mining, both in practical use and in research. As before, we have a set of samples that represents objects or things we are interested in classifying. We also have a new array, the class values. These class values give us a categorization of the samples. Some examples are as follows:\n",
    "\n",
    "- Determining the species of a plant by looking at its measurements. The class value here would be *Which species is this?*.\n",
    "- Determining if an image contains a dog. The class would be *Is there a dog in this image?*.\n",
    "- Determining if a patient has cancer based on the test results. The class would be *Does this patient have cancer?*.\n",
    "\n",
    "The goal of classification applications is to train a model on a set of samples with known classes, and then apply that model to new unseen samples with unknown classes. For example, we want to train a spam classifier on my past e-mails, which I have labeled as spam or not spam. I then want to use that classifier to determine whether my next email is spam, without me needing to classify it myself.\n",
    "\n",
    "## Loading and preparing the dataset\n",
    "\n",
    "The dataset we are going to use for this example is the famous **Iris database** of plant\n",
    "classification.\n",
    "\n",
    "We have 150 plant samples and four measurements of each (all in centimeters): \n",
    "- **sepal length**\n",
    "- **sepal width**\n",
    "- **petal length**\n",
    "- **petal width**\n",
    "\n",
    "This classic dataset (first used in 1936!) is one of the classic datasets for data mining.\n",
    "There are three classes: \n",
    "- **Iris Setosa**\n",
    "- **Iris Versicolour**\n",
    "- **Iris Virginica**\n",
    "\n",
    "The aim is to determine which type of plant a sample is, by examining its measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "dataset = load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "\n",
    "print(dataset.DESCR)\n",
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this dataset are continuous values, meaning they can take any range of values. Measurements are a good example of this type of feature, where a measurement can take the value of 1, 1.2, or 1.25 and so on. Another aspect about continuous features is that feature values that are close to each other indicate similarity. A plant with a sepal length of 1.2 cm is like a plant with sepal width of 1.25 cm.\n",
    "\n",
    "In contrast are categorical features. These features, while often represented as numbers, cannot be compared in the same way. In the Iris dataset, the class values are an example of a categorical feature. \n",
    "\n",
    "- class 0 represents Iris Setosa\n",
    "- class 1 represents Iris Versicolour\n",
    "- class 2 represents Iris Virginica \n",
    "\n",
    "This doesn't mean that Iris Setosa is more similar to Iris Versicolour than it is to Iris Virginica. The numbers here represent categories. All we can say is whether categories are the same or different.\n",
    "\n",
    "## Implementing the OneR algorithm\n",
    "\n",
    "OneR is a shorthand for *One Rule*, indicating we only use a single rule for this classification by choosing the feature with the best performance. This simple algorithm has been shown to have good performance in a\n",
    "number of real-world datasets.\n",
    "\n",
    "Our attributes are continuous, while we want categorical features to use OneR. We will perform a preprocessing step called discretisation. At this stage, we will perform a simple procedure: compute the mean and determine whether a value is above or below the mean.\n",
    "\n",
    "### The algorithm \n",
    "- Start by iterating over every value of every feature\n",
    "- For that value, count the number of samples from each class that have that feature value\n",
    "- Record the most frequent class for the feature value, and the error of that prediction\n",
    "\n",
    "    - **For example**: If a feature has two values, 0 and 1, we first check all samples that have the value 0. For that value, we may have 20 in class A, 60 in class B, and a further 20 in class C. The most frequent class for this value is B, and there are 40 instances that have difference classes. The prediction for this feature value is B with an error of 40, as there are 40 samples that have a different class from the prediction. We then do the same procedure for the value 1 for this feature, and then for all other feature value combinations.\n",
    "\n",
    "- We compute the error for each feature by summing up the errors for all values for that feature\n",
    "- The feature with the lowest total error is chosen as the One Rule and then used to classify other instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the mean for each attribute\n",
    "attribute_means = X.mean(axis=0)\n",
    "assert attribute_means.shape == (n_features,)\n",
    "X_d = np.array(X >= attribute_means, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are (112,) training samples\n",
      "There are (38,) testing samples\n"
     ]
    }
   ],
   "source": [
    "# Now, we split into a training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the random state to the same number to get the same results everytime\n",
    "random_state = 14\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d, y, random_state=random_state)\n",
    "print(\"There are {} training samples\".format(y_train.shape))\n",
    "print(\"There are {} testing samples\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def train(X, y_true, feature):\n",
    "    \"\"\"Computes the predictors and error for a given feature using the OneR algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array [n_samples, n_features]\n",
    "        The two dimensional array that holds the dataset. Each row is a sample, each column\n",
    "        is a feature.\n",
    "    \n",
    "    y_true: array [n_samples,]\n",
    "        The one dimensional array that holds the class values. Corresponds to X, such that\n",
    "        y_true[i] is the class value for sample X[i].\n",
    "    \n",
    "    feature: int\n",
    "        An integer corresponding to the index of the variable we wish to test.\n",
    "        0 <= variable < n_features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    predictors: dictionary of tuples: (value, prediction)\n",
    "        For each item in the array, if the variable has a given value, make the given prediction.\n",
    "    \n",
    "    error: float\n",
    "        The ratio of training data that this rule incorrectly predicts.\n",
    "    \"\"\"\n",
    "    # Check that variable is a valid number\n",
    "    n_samples, n_features = X.shape\n",
    "    assert 0 <= feature < n_features\n",
    "    # Get all of the unique values that this variable has\n",
    "    values = set(X[:,feature])\n",
    "    # Stores the predictors array that is returned\n",
    "    predictors = dict()\n",
    "    errors = []\n",
    "    for current_value in values:\n",
    "        most_frequent_class, error = train_feature_value(X, y_true, feature, current_value)\n",
    "        predictors[current_value] = most_frequent_class\n",
    "        errors.append(error)\n",
    "    # Compute the total error of using this feature to classify on\n",
    "    total_error = sum(errors)\n",
    "    return predictors, total_error\n",
    "\n",
    "# Compute what our predictors say each sample is based on its value\n",
    "#y_predicted = np.array([predictors[sample[feature]] for sample in X])\n",
    "    \n",
    "\n",
    "def train_feature_value(X, y_true, feature, value):\n",
    "    # Create a simple dictionary to count how frequency they give certain predictions\n",
    "    class_counts = defaultdict(int)\n",
    "    # Iterate through each sample and count the frequency of each class/value pair\n",
    "    for sample, y in zip(X, y_true):\n",
    "        if sample[feature] == value:\n",
    "            class_counts[y] += 1\n",
    "    # Now get the best one by sorting (highest first) and choosing the first item\n",
    "    sorted_class_counts = sorted(class_counts.items(), key=itemgetter(1), reverse=True)\n",
    "    most_frequent_class = sorted_class_counts[0][0]\n",
    "    # The error is the number of samples that do not classify as the most frequent class\n",
    "    # *and* have the feature value.\n",
    "    n_samples = X.shape[1]\n",
    "    error = sum([class_count for class_value, class_count in class_counts.items()\n",
    "                 if class_value != most_frequent_class])\n",
    "    return most_frequent_class, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is based on variable 2 and has error 37.00\n",
      "{'variable': 2, 'predictor': {0: 0, 1: 2}}\n"
     ]
    }
   ],
   "source": [
    "# Compute all of the predictors\n",
    "all_predictors = {variable: train(X_train, y_train, variable) for variable in range(X_train.shape[1])}\n",
    "errors = {variable: error for variable, (mapping, error) in all_predictors.items()}\n",
    "# Now choose the best and save that as \"model\"\n",
    "# Sort by error\n",
    "best_variable, best_error = sorted(errors.items(), key=itemgetter(1))[0]\n",
    "print(f\"The best model is based on variable {best_variable} and has error {best_error:.2f}\")\n",
    "\n",
    "# Choose the bset model\n",
    "model = {'variable': best_variable,\n",
    "         'predictor': all_predictors[best_variable][0]}\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    variable = model['variable']\n",
    "    predictor = model['predictor']\n",
    "    y_predicted = np.array([predictor[int(sample[variable])] for sample in X_test])\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 2 2 2 0 2 0 2 2 0 2 2 0 2 0 2 2 2 0 0 0 2 0 2 0 2 2 0 0 0 2 0 2 0 2\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = predict(X_test, model)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the algorithm\n",
    "\n",
    "When we evaluated the affinity analysis algorithm of the last section, our aim was to explore the current dataset. With this classification, our problem is different. We want to build a model that will allow us to classify previously unseen samples by comparing them to what we know about the problem.\n",
    "\n",
    "For this reason, we split our machine-learning workflow into two stages: training and testing. \n",
    "- In training, we take a portion of the dataset and create our model\n",
    "- In testing, we apply that model and evaluate how effectively it worked on the dataset. \n",
    "\n",
    "As our goal is to create a model that is able to classify previously unseen samples, we cannot use our testing data for training the model. If we do, we run the risk of **overfitting** (the problem of creating a model that classifies our training dataset very well, but performs poorly on new samples).\n",
    "\n",
    "#### The rule\n",
    "\n",
    "Never use training data to test your algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 65.8%\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy by taking the mean of the amounts that y_predicted is equal to y_test\n",
    "accuracy = np.mean(y_predicted == y_test) * 100\n",
    "print(\"The test accuracy is {:.1f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        17\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.40      1.00      0.57         8\n",
      "\n",
      "    accuracy                           0.66        38\n",
      "   macro avg       0.45      0.67      0.51        38\n",
      "weighted avg       0.51      0.66      0.55        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predicted, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
