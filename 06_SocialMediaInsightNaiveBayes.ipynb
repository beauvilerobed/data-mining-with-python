{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Insight Using Naive Bayes\n",
    "\n",
    "\n",
    "Text-based datasets contain a lot of information, whether they are books, historical documents, social media, e-mail, or any of the other ways we communicate via writing. Extracting features from text-based datasets and using them for classification is a difficult problem. There are, however, some common patterns for text mining. We look at disambiguating terms in social media using the Naive Bayes algorithm, which is a powerful and surprisingly simple algorithm. Naive Bayes takes a few shortcuts to properly compute the probabilities for classification, hence the term naive in the name. It can also be extended to other types of datasets quite easily and doesn't rely on numerical features. The model is a baseline for text mining studies, as the process can work reasonably well for a variety of datasets.\n",
    "\n",
    "We will cover the following topics in this chapter:\n",
    "- Downloading data from social network APIs\n",
    "- Transformers for text\n",
    "- Naive Bayes classifier\n",
    "- Using JSON for saving and loading datasets\n",
    "- The NLTK library for extracting features from text\n",
    "- The F-measure for evaluation\n",
    "\n",
    "## Disambiguation\n",
    "\n",
    "Text is often called an **unstructured** format. There is a lot of information there, but it is just there; no headings, no required format, loose syntax and other problems prohibit the easy extraction of information from text. The data is also highly connected, with lots of mentions and cross-references—just not in a format that allows us to easily extract it!\n",
    "\n",
    "We can compare the information stored in a book with that stored in a large database to see the difference. In the book, there are characters, themes, places, and lots of information. However, the book needs to be read and, more importantly, interpreted to gain this information. The database sits on your server with column names and data types. All the information is there and the level of interpretation needed is quite low. Information about the data, such as its type or meaning is called **metadata**, and text lacks it. A book also contains some metadata in the form of a table of contents and index but the degree is significantly lower than that of a database. \n",
    "\n",
    "One of the problems is the term **disambiguation**. When a person uses the word bank, is this a financial message or an environmental message (such as river bank)? This type of disambiguation is quite easy in many circumstances for humans (although there are still troubles), but much harder for computers to do.\n",
    "\n",
    "Here, we will look at disambiguating the use of the term Python on Twitter's stream. A message on Twitter is called a tweet and is limited to 140 characters. This means there is little room for context. There isn't much metadata available although hashtags are often used to denote the topic of the tweet.\n",
    "\n",
    "When people talk about Python, they could be talking about the following things:\n",
    "- The programming language Python\n",
    "- Monty Python, the classic comedy group\n",
    "- The snake Python\n",
    "- A make of shoe called Python\n",
    "\n",
    "There can be many other things called Python. The aim of our experiment is to take a tweet mentioning Python and determine whether it is talking about the programming language, based only on the content of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data from a social network\n",
    "\n",
    "We are going to download a corpus of data from Twitter and use it to sort out spam from useful content. Twitter provides a robust API for collecting information from its servers and this API is free for small-scale usage. It is, however, subject to some conditions that you'll need to be aware of if you start using Twitter's data in a commercial setting.\n",
    "\n",
    "First, you'll need to sign up for a Twitter account (which is free). Go to http://twitter.com and register an account if you do not already have one.\n",
    "\n",
    "Next, you'll need to ensure that you only make a certain number of requests per minute. This limit is currently 180 requests per hour. It can be tricky ensuring that you don't breach this limit, so it is highly recommended that you use a library to talk to Twitter's API.\n",
    "\n",
    "You will need a key to access Twitter's data. Go to http://twitter.com and sign in to your account.\n",
    "\n",
    "When you are logged in, go to https://apps.twitter.com/ and click on Create New App.\n",
    "\n",
    "Create a name and description for your app, along with a website address. If you don't have a website to use, insert a placeholder. Leave the Callback URL field blank for this app—we won't need it. Agree to the terms of use (if you do) and click on Create your Twitter application.\n",
    "\n",
    "Keep the resulting website open—you'll need the access keys that are on this page. Next, we need a library to talk to Twitter. There are many options; the one I like is simply called twitter, and is the official Twitter Python library.\n",
    "\n",
    "Note: You can install twitter using pip3 install twitter if you are using pip to install your packages. If you are using another system,check the documentation at https://github.com/sixohsix/ twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import twitter\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# consumer_key = \"<Your Consumer Key Here>\"\n",
    "# consumer_secret = \"<Your Consumer Secret Here>\"\n",
    "# access_token = \"<Your Access Token Here>\"\n",
    "# access_token_secret = \"<Your Access Token Secret Here>\"\n",
    "# authorization = twitter.OAuth(access_token, access_token_secret,\n",
    "# consumer_key, consumer_secret)\n",
    "\n",
    "# output_filename = os.path.join(os.path.expanduser(\"~\"),\n",
    "#  \"Data\", \"twitter\", \"python_tweets.json\")\n",
    "\n",
    "\n",
    "# t = twitter.Twitter(auth=authorization)\n",
    "\n",
    "# with open(output_filename, 'a') as output_file:\n",
    "#     search_results = t.search.tweets(q=\"python\", count=100)['statuses']\n",
    "#     for tweet in search_results:\n",
    "#         if 'text' in tweet:\n",
    "#             output_file.write(json.dumps(tweet))\n",
    "#             output_file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding loop, we also perform a check to see whether there is text in the tweet or not. Not all of the objects returned by twitter will be actual tweets (some will be actions to delete tweets and others). The key difference is the inclusion of text as a key, which we test for.\n",
    "\n",
    "Running this for a few minutes will result in 100 tweets being added to the\n",
    "output file.\n",
    "\n",
    "## Loading and classifying the dataset\n",
    "\n",
    "After we have collected a set of tweets (our dataset), we need labels to perform classification. The dataset we have stored is nearly in a JSON format. JSON is a format for data that doesn't impose much structure and is directly readable in JavaScript (hence the name, JavaScript Object Notation). JSON defines basic objects such as numbers, strings, lists and dictionaries, making it a good format for storing datasets if they contain data that isn't numerical. If your dataset is fully numerical, you would save space and time using a matrix-based format like in NumPy.\n",
    "\n",
    "To parse it, we can use the json library but we will have to first split the file by newlines to get the actual tweet objects themselves.\n",
    "\n",
    "## Loading data without the twitterAPI\n",
    "We do not need to used the twitterAPI or anything of the like. There is a saved .txt file that has 100 tweets that we will use. If you wish to use the twitterAPI feel free to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'data/social-media-data/'\n",
    "TWITTER = DATA + 'posts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no user: id = 29 \n",
      " tweet: Available Now At -… http://t.co/tvgQugZZT5 \n",
      "\n",
      "no user: id = 31 \n",
      " tweet: No_not_that_one\n",
      "RT @SamuelHLowe: - Excuse me, do you have snake belts?\n",
      "- Not sure, but I can check in the back.\n",
      "- Please do, my python's pants keep falling… \n",
      "\n",
      "no user: id = 33 \n",
      " tweet: .....))))))))))) \n",
      "\n",
      "no user: id = 51 \n",
      " tweet: halulan\n",
      "@python_octopus \n",
      "乙です！！ \n",
      "\n",
      "Loaded 102 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets_list = []\n",
    "with open(TWITTER, \"r\") as file:\n",
    "    content = file.read().split('\\n\\n')\n",
    "    for i, line in enumerate(content):\n",
    "        try: \n",
    "            user, tweet = line.split('\\n')\n",
    "            tweets_list.append([user, tweet])\n",
    "        except:\n",
    "            print(f'no user: id = {i} \\n tweet: {line} \\n')\n",
    "            tweets_list.append(['no user', tweet])\n",
    "\n",
    "print(f\"Loaded {len(tweets_list)} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in classifying whether an item is relevant to us or not (in this case, relevant means refers to the programming language Python). We will use the IPython Notebook's ability to embed HTML and talk between JavaScript and Python to create a viewer of tweets to allow us to easily and quickly classify the tweets as spam or not.\n",
    "\n",
    "The code will present a new tweet to the user (you) and ask for a label: is it relevant\n",
    "or not? It will then store the input and present the next tweet to be labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = []\n",
    "\n",
    "# print(f\"\"\" Instructions: \n",
    "#            Enter a 1 if the tweet is relevant, enter 2 otherwise.\"\"\")\n",
    "\n",
    "# n = len(tweets_list)+1\n",
    "# idx = 0\n",
    "# while len(labels) <= n:\n",
    "#     print(f\"\"\"Tweet: {tweets_list[idx][1]}\"\"\")\n",
    "#     a = input()\n",
    "#     try:\n",
    "#         if a == 'exit':\n",
    "#             break\n",
    "#         val = int(a)\n",
    "#         if val in [1,2]:\n",
    "#             labels.append(int(a))\n",
    "#             idx += 1\n",
    "#         else:\n",
    "#             print('invalid input: must be 1 or 2')\n",
    "#     except:\n",
    "#         print('invalid input: must be 1 or 2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# # Open a file in write mode\n",
    "# with open(DATA+\"labels.txt\", \"w\") as f:\n",
    "#     # Write the array string to the file\n",
    "#     my_array = np.array(labels)\n",
    "#     array_string = np.array2string(my_array)\n",
    "#     f.write(array_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a replicable dataset from Twitter\n",
    "\n",
    "In data mining, there are lots of variables. These aren't just in the data mining algorithms—they also appear in the data collection, environment, and many other factors. Being able to replicate your results is important as it enables you to verify or improve upon your results.\n",
    "\n",
    "**Note:** Getting 80 percent accuracy on one dataset with algorithm X, and 90 percent accuracy on another dataset with algorithm Y doesn't mean that Y is better. We need to be able to test on the same dataset in the same conditions to be able to properly compare.\n",
    "\n",
    "Your labeling of tweets might be different from what is here. While there are obvious examples where a given tweet relates to the python programming language, there will always be gray areas where the labeling isn't obvious. One tough gray area was tweets in non-English languages that couldn't be read.\n",
    "\n",
    "Due to these factors, it is difficult to replicate experiments on databases that are extracted from social media, and Twitter is no exception. Twitter explicitly disallows sharing datasets directly.\n",
    "\n",
    "One solution to this is to share tweet IDs only, which you can share freely. In this section, we will first create a simulation of tweet ID dataset that we can theoretically be freely share, named `label.txt` in our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "# a label with -1 is n/a\n",
    "with open(DATA+'labels.txt', 'r') as f:\n",
    "    content = f.read()[1:-1]\n",
    "    label_vals = content.split('\\n')\n",
    "    for val in label_vals:\n",
    "        for label in val.split():\n",
    "            labels.append(int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "n_samples = min(len(tweets_list), len(labels))\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tweet IDs and labels, we can recreate the original dataset if we wanted to but we will not do that here since the code up until now creates the dataset for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text transformers\n",
    "\n",
    "Now that we have our dataset, how are we going to perform data mining on it?\n",
    "\n",
    "Text-based datasets include books, essays, websites, manuscripts, programming code, and other forms of written expression. All of the algorithms we have seen so far deal with numerical or categorical features, so how do we convert our text into a format that the algorithm can deal with?\n",
    "\n",
    "There are a number of measurements that could be taken. For instance, average word and average sentence length are used to predict the readability of a document. However, there are lots of feature types such as word occurrence which we will now investigate.\n",
    "\n",
    "### Bag-of-words\n",
    "\n",
    "One of the simplest but highly effective models is to simply count each word in the dataset. We create a matrix, where each row represents a document in our dataset and each column represents a word. The value of the cell is the frequency of that word in the document.\n",
    "\n",
    "Here's an excerpt from The Lord of the Rings, J.R.R. Tolkien:\n",
    "\n",
    "```\n",
    "Three Rings for the Elven-kings under the sky,\n",
    "Seven for the Dwarf-lords in halls of stone,\n",
    "Nine for Mortal Men, doomed to die,\n",
    "One for the Dark Lord on his dark throne\n",
    "In the Land of Mordor where the Shadows lie.\n",
    "One Ring to rule them all, One Ring to find them,\n",
    "One Ring to bring them all and in the darkness bind them.\n",
    "In the Land of Mordor where the Shadows lie.\n",
    " - J.R.R. Tolkien's epigraph to The Lord of The Rings\n",
    "```\n",
    "\n",
    "The word the appears nine times in this quote, while the words in, for, to, and one each appear four times. The word ring appears three times, as does the word of. \n",
    "\n",
    "We can create a dataset from this, choosing a subset of words and counting the frequency:\n",
    "\n",
    "|Word| the| one| ring| to|\n",
    "|--|--|--|--|--|\n",
    "|Frequency| 9| 4| 3| 4|\n",
    "\n",
    "We can use the counter class to do a simple count for a given string. When counting words, it is normal to convert all letters to lowercase, which we do when creating thestring. The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 9), ('for', 4), ('in', 4), ('to', 4), ('one', 4)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"Three Rings for the Elven-kings under the sky,\n",
    "Seven for the Dwarf-lords in halls of stone,\n",
    "Nine for Mortal Men, doomed to die,\n",
    "One for the Dark Lord on his dark throne\n",
    "In the Land of Mordor where the Shadows lie.\n",
    "One Ring to rule them all, One Ring to find them,\n",
    "One Ring to bring them all and in the darkness bind them.\n",
    "In the Land of Mordor where the Shadows lie. \"\"\".lower()\n",
    "words = s.split()\n",
    "from collections import Counter\n",
    "c = Counter(words)\n",
    "c.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing `c.most_common(5)` gives the list of the top five most frequently occurring words. Ties are not handled well as only five are given and a very large number of words all share a tie for fifth place.\n",
    "\n",
    "The bag-of-words model has three major types. \n",
    "- The first is to use the raw frequencies, as shown in the preceding example. This does have a drawback when documents vary in size from fewer words to many words, as the overall values will be very different. \n",
    "- The second model is to use the **normalized frequency**, where each document's sum equals 1. This is a much better solution as the length of the document doesn't matter as much. \n",
    "- The third type is to simply use binary features—a value is 1 if the word occurs *at all* and 0 if it doesn't. We will use binary representation in this case.\n",
    "\n",
    "Another popular (arguably more popular) method for performing normalization is called **term frequency - inverse document frequency**, or **tf-idf**. In this weighting scheme, term counts are first normalized to frequencies and then divided by the number of documents in which it appears in the corpus.\n",
    "\n",
    "There are a number of libraries for working with text data in Python. We will use a major one, called **Natural Language ToolKit (NLTK)**. The `scikit-learn` library also has the `CountVectorizer` class that performs a similar action, and it is recommended you take a look at it. However the NLTK version has more options for word tokenization. If you are doing natural language processing in python, NLTK is a great library to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "A step up from single bag-of-words features is that of **n-grams**. An n-gram is a\n",
    "subsequence of n consecutive tokens.\n",
    "\n",
    "They are counted the same way, with the n-grams forming a word that is put in the bag. The value of a cell in this dataset is the frequency that a particular n-gram appears in the given document.\n",
    "\n",
    "**Note:** The value of n is a parameter. For English, setting it to between 2 to 5 is a good start, although some applications call for higher values\n",
    "\n",
    "As an example, for n=3, we extract the first few n-grams in the following quote:\n",
    "\n",
    "*Always look on the bright side of life.*\n",
    "\n",
    "The first n-gram (of size 3) is Always look on, the second is look on the, the third is on the bright. As you can see, the n-grams overlap and cover three words.\n",
    "\n",
    "Word n-grams have advantages over using single words. This simple concept introduces some context to word use by considering its local environment, without a large overhead of understanding the language computationally. A disadvantage of using n-grams is that the matrix becomes even sparser—word n-grams are unlikely to appear twice (especially in tweets and other short documents!).\n",
    "\n",
    "Specially for social media and other short documents, word n-grams are unlikely to appear in too many different tweets, unless it is a retweet. However, in larger documents, word n-grams are quite effective for many applications.\n",
    "\n",
    "Another form of n-gram for text documents is that of a **character n-gram**. Rather than using sets of words, we simply use sets of characters (although character n-grams have lots of options for how they are computed!). **This type of dataset can pick up words that are misspelled, as well as providing other benefits**.\n",
    "\n",
    "## Other features (further reading)\n",
    "\n",
    "There are other features that can be extracted too. These include syntactic features, such as the usage of particular words in sentences. Part-of-speech tags are also popular for data mining applications that need to understand meaning in text. Such feature types won't be covered in this book. If you are interested in learning more:\n",
    "- *Python 3 Text Processing with NLTK 3 Cookbook, Jacob Perkins, Packt publication.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic model that is unsurprisingly built upon a naive interpretation of Bayesian statistics. Despite the naive aspect, the method performs very well in a large number of contexts. It can be used for classification of many different feature types and formats, but we will focus on one: binary features in the bag-of-words model.\n",
    "\n",
    "## Bayes' theorem\n",
    "\n",
    "For most of us, when we were taught statistics, we started from a frequentist approach. In this approach, we assume the data comes from some distribution and we aim to determine what the parameters are for that distribution. However, those parameters are (perhaps incorrectly) assumed to be fixed. We use our model to describe the data, even testing to ensure the data fits our model.\n",
    "\n",
    "Bayesian statistics instead model how people (non-statisticians) actually reason. We have some data and we use that data to update our model about how likely something is to occur. In Bayesian statistics, we use the data to describe the model rather than using a model and confirming it with data (as per the frequentist approach).\n",
    "\n",
    "Bayes' theorem computes the value of $P(A|B)$, that is, knowing that $B$ has occurred,  what is the probability of $A$. In most cases, $B$ is an observed event such as it rained yesterday, and $A$ is a prediction it will rain today. For data mining, $B$ is usually we observed this sample and $A$ is it belongs to this class. We will see how to use Bayes' theorem for data mining in the next section. The equation for Bayes' theorem is given as follows:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "As an example, we want to determine the probability that an e-mail containing the word drugs is spam (as we believe that such a tweet may be a pharmaceutical spam). $A$, in this context, is the probability that this tweet is spam. We can compute $P(A)$, called the prior belief directly from a training dataset by computing the percentage of tweets in our dataset that are spam. If our dataset contains 30 spam messages for every 100 e-mails, $P(A) =  0.3$.\n",
    "\n",
    "$B$, in this context, is this tweet contains the word 'drugs'. Likewise, we can compute $P(B)$ by computing the percentage of tweets in our dataset containing the word drugs. If 10 e-mails in every 100 of our training dataset contain the word drugs, $P(B) = 0.1$. Note that we don't care if the e-mail is spam or not when computing this value. \n",
    "\n",
    "$P(B|A)$ is the probability that an e-mail contains the word drugs if it is spam. It is also easy to compute from our training dataset. We look through our training set for spam e-mails and compute the percentage of them that contain the word drugs. Of our 30 spam e-mails, if 6 contain the word drugs, then $P(B|A) = 0.2$.\n",
    "\n",
    "From here, we use Bayes' theorem to compute P(A|B), which is the probability that\n",
    "a tweet containing the word drugs is spam. Using the previous equation, we see the\n",
    "result is 0.6. This indicates that if an e-mail has the word drugs in it, there is a 60\n",
    "percent chance that it is spam.\n",
    "\n",
    "Note the empirical nature of the preceding example—we use evidence directly from our training dataset, not from some preconceived distribution. In contrast, a frequentist view of this would rely on us creating a distribution of the probability of words in tweets to compute similar equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes algorithm\n",
    "\n",
    "Looking back at our Bayes' theorem equation, we can use it to compute the probability that a given sample belongs to a given class. This allows the equation to be used as a classification algorithm.\n",
    "\n",
    "With $C$ as a given class and $D$ as a sample in our dataset, we create the elements necessary for Bayes' theorem, and subsequently Naive Bayes. Naive Bayes is a classification algorithm that utilizes Bayes' theorem to compute the probability that a new data sample belongs to a particular class.\n",
    "\n",
    "$P(C)$ is the probability of a class, which is computed from the training dataset itself (as we did with the spam example). We simply compute the percentage of samples in our training dataset that belong to the given class.\n",
    "\n",
    "$P(D)$ is the probability of a given data sample. It can be difficult to compute this, as the sample is a complex interaction between different features, but luckily it is a constant across all classes. Therefore, we don't need to compute it at all. We will see later how to get around this issue.\n",
    "\n",
    "$P(D|C)$ is the probability of the data point belonging to the class. This could also be difficult to compute due to the different features. However, this is where we **introduce the naive part of the Naive Bayes algorithm**. We naively assume that each feature is independent of each other. Rather than computing the full probability of $P(D|C)$, we compute the probability of each feature $D_1, D_2, D_3, …$ and so on. Then,\n",
    "we multiply them together:\n",
    "\n",
    "$$\n",
    "P(D|C) = P(D_1|C)P(D_2|C)\\cdots P(D_n|C)\n",
    "$$\n",
    "\n",
    "Each of these values is relatively easy to compute with binary features; we simply compute the percentage of times it is equal in our sample dataset.\n",
    "\n",
    "In contrast, if we were to perform a non-naive Bayes version of this part, we would need to compute the correlations between different features for each class. Such computation is infeasible at best, and nearly impossible without vast amounts of data\n",
    "or adequate language analysis models.\n",
    "\n",
    "From here, the algorithm is straightforward. We compute $P(C|D)$ for each possible class, ignoring the $P(D)$ term. Then we choose the class with the highest probability. As the P(D) term is consistent across each of the classes, ignoring it has no impact on the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "\n",
    "As an example, suppose we have the following (binary) feature values from a sample in our dataset: $D = [1, 0, 0, 1]$.\n",
    "\n",
    "Our training dataset contains two classes with 75 percent of samples belonging to the class 0, and 25 percent belonging to the class 1. The likelihood of the feature values for each class are as follows:\n",
    "\n",
    "For class 0: $[P(D_1=1|C=0),P(D_2=1|C=0),P(D_3=1|C=0),P(D_4=1|C=0)] = [0.3, 0.4, 0.4, 0.7]$\n",
    "\n",
    "For class 1: $[P(D_1=1|C=1),P(D_2=1|C=1),P(D_3=1|C=1),P(D_4|=1C=1)] = [0.7, 0.3, 0.4, 0.9]$\n",
    "\n",
    "These values are to be interpreted as: for feature 1, $D_1$, it is equal to 1 in 30 percent of cases for class 0, i.e. $P(D_1=1|C=0) = .3$.\n",
    "\n",
    "We can now compute the probability that this sample should belong to the class 0:\n",
    "\n",
    "$P(C=0) = 0.75$ which is the probability that the class is 0.\n",
    "\n",
    "Note that $P(D)$ isn't needed for the Naive Bayes algorithm. Let's take a look at the calculation:\n",
    "\n",
    "$P(D|C=0) = P(D_1|C=0)P(D_2|C=0)P(D_3|C=0)P(D_4|C=0) = 0.3\\cdot 0.6 \\cdot 0.6 \\cdot 0.7= 0.0756$\n",
    "\n",
    "**Note:** The listed probabilities are for values of 1 for each feature. Therefore, the probability of a 0 is its inverse: $P(0) = 1 – P(1)$.\n",
    "\n",
    "Now, we can compute the probability of the data point belonging to this class. An important point to note is that we haven't computed P(D), so this isn't a real probability. However, it is good enough to compare against the same value for the probability of the class 1. Let's take a look at the calculation:\n",
    "\n",
    "$$\n",
    "P(C=0|D) = \\frac{P(C=0) P(D|C=0)}{P(D)}\n",
    "= \\frac{0.75 \\cdot 0.0756}{P(D)}\n",
    "= \\frac{0.0567}{P(D)}\n",
    "$$\n",
    "\n",
    "Now, we compute the same values for the class 1: $P(C=1) = 0.25$\n",
    "\n",
    "$P(D)$ isn't needed for naive Bayes. Let's take a look at the calculation:\n",
    "\n",
    "$P(D|C=1) = P(D_1|C=1)P(D_2|C=1)P(D_3|C=1)P(D_4|C=1)\n",
    "= 0.7 \\cdot 0.7 \\cdot 0.6 \\cdot 0.9\n",
    "= 0.2646$\n",
    "\n",
    "$$\n",
    "P(C=1|D) = \\frac{P(C=1) P(D|C=1)}{P(D)}\n",
    "= \\frac{0.25 \\cdot 0.2646}{P(D)}\n",
    "= \\frac{0.06615}{P(D)}\n",
    "$$\n",
    "\n",
    "The data point should be classified as belonging to the class 1. You may have guessed this while going through the equations anyway; however, you may have been a bit surprised that the final decision was so close. After all, the probabilities in computing P(D|C) were much, much higher for the class 1. This is because we introduced a prior belief that most samples generally belong to the class 0.\n",
    "\n",
    "If the classes had been equal sizes, the resulting probabilities would be much different. Try it yourself by changing both $P(C=0)$ and $P(C=1)$ to 0.5 for equal class sizes and computing the result again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "We will now create a pipeline that takes a tweet and determines whether it is relevant or not, based only on the content of that tweet. To perform the word extraction, we will be using the NLTK, a library that contains a large number of tools for performing analysis on natural language.\n",
    "\n",
    "We are going to create a pipeline to extract the word features and classify the tweets using Naive Bayes. Our pipeline has the following steps:\n",
    "\n",
    "1. Transform the original text documents into a dictionary of counts using NLTK's word_tokenize function.\n",
    "2. Transform those dictionaries into a vector matrix using the DictVectorizer transformer in scikit-learn. This is necessary to enable the Naive Bayes classifier to read the feature values extracted in the first step.\n",
    "3. Train the Naive Bayes classifier.\n",
    "\n",
    "## Extracting word counts\n",
    "\n",
    "We are going to use NLTK to extract our word counts. We still want to use it in a pipeline, but NLTK doesn't conform to our transformer interface. We will therefore need to create a basic transformer to do this to obtain both fit and transform methods, enabling us to use this in a pipeline.\n",
    "\n",
    "First, set up the transformer class. We don't need to fit anything in this class, as this transformer simply extracts the words in the document. Therefore, our fit is an empty function, except that it returns self which is necessary for transformer objects. Our transform is a little more complicated. We want to extract each word from each document and record True if it was discovered. We are only using the binary features here—True if in the document, False otherwise. If we wanted to use the frequency we would set up counting dictionaries, as we have done in several of the past chapters.\n",
    "\n",
    "Let's take a look at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/robed/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [{word: True for word in word_tokenize(document)}\n",
    "                 for document in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dictionaries to a matrix\n",
    "\n",
    "This step converts the dictionaries built as per the previous step into a matrix that can be used with a classifier. This step is made quite simple through the `DictVectorizer` transformer.\n",
    "\n",
    "The `DictVectorizer` class simply takes a list of dictionaries and converts them into a matrix. The features in this matrix are the keys in each of the dictionaries, and the values correspond to the occurrence of those features in each sample. Dictionaries are easy to create in code, but many data algorithm implementations prefer matrices. This makes `DictVectorizer` a very useful class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes classifier\n",
    "\n",
    "Finally, we need to set up a classifier and we are using Naive Bayes for this section. As our dataset contains only binary features, we use the `BernoulliNB` classifier that is designed for binary features. As a classifier, it is very easy to use. As with `DictVectorizer`, we simply import it and add it to our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now, create a pipeline putting together the components from before. Our pipeline\n",
    "has three parts:\n",
    "- The `NLTKBOW` transformer we created\n",
    "- A `DictVectorizer` transformer\n",
    "- A `BernoulliNB` classifier\n",
    "The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('bag-of-words', NLTKBOW()),\n",
    "                     ('vectorizer', DictVectorizer()),\n",
    "                     ('naive-bayes', BernoulliNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can nearly run our pipeline now, which we will do with `cross_val_score` as we have done many times before. Before that though, we will introduce a better evaluation metric than the accuracy metric we used before. As we will see, the use of accuracy is not adequate for datasets when the number of samples in each class is different.\n",
    "\n",
    "## Evaluation using the F1-score\n",
    "When choosing an evaluation metric, it is always important to consider cases where that evaluation metric is not useful. Accuracy is a good evaluation metric in many cases, as it is easy to understand and simple to compute. However, it can be easily faked. In other words, in many cases you can create algorithms that have a high accuracy by poor utility.\n",
    "\n",
    "While our dataset of tweets (typically, your results may vary) contains about 50 percent programming-related and 50 percent nonprogramming (see below), many datasets aren't as **balanced** as this.\n",
    "\n",
    "As an example, an e-mail spam filter may expect to see more than 80 percent of incoming e-mails be spam. A spam filter that simply labels everything as spam is quite useless; however, it will obtain an accuracy of 80 percent!\n",
    "\n",
    "To get around this problem, we can use other evaluation metrics. One of the most commonly employed is called an f1-score (also called f-score, f-measure, or one of many other variations on this term).\n",
    "\n",
    "The f1-score is defined on a per-class basis and is based on two concepts: the precision and recall. The **precision** is the percentage of all the samples that were predicted as belonging to a specific class that were actually from that class. The **recall** is the percentage of samples in the dataset that are in a class and actually labeled as belonging to that class.\n",
    "\n",
    "In the case of our application, we could compute the value for both classes (relevant and not relevant). However, we are really interested in the spam. Therefore, our precision computation becomes the question: of all the tweets that were predicted as being relevant, what percentage were actually relevant? Likewise, the recall becomes the question: of all the relevant tweets in the dataset, how many were predicted as being relevant?\n",
    "\n",
    "After you compute both the precision and recall, the f1-score is the harmonic mean of the precision and recall:\n",
    "\n",
    "$$\n",
    "F_1 = 2\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "$$\n",
    "\n",
    "To use the f1-score in scikit-learn methods, simply set the scoring parameter to f1. By default, this will return the f1-score of the class with label 1. Running the code on our dataset, we simply use the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.00%% have class 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.mean(labels == 1):.2%}% have class 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = np.array([tweet[1] for tweet in tweets_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "scores = cross_val_score(pipeline, tweets, labels, scoring='f1')\n",
    "print(f\"Score: {np.mean(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 0.711, which means we can accurately determine if a tweet using Python relates to the programing language 70 percent of the time. This is using a dataset with only 102 tweets in it.\n",
    "\n",
    "## Getting useful features from models\n",
    "\n",
    "One question you may ask is what are the best features for determining if a tweet is relevant or not? We can extract this information from of our Naive Bayes model and find out which features are the best individually, according to Naive Bayes. First we fit a new model. While the cross_val_score gives us a score across different folds of cross-validated testing data, it doesn't easily give us the trained models themselves. To do this, we simply fit our pipeline with the tweets, creating a new model.\n",
    "\n",
    "A pipeline gives you access to the individual steps through the named_steps attribute and the name of the step (we defined these names ourselves when we created the pipeline object itself). For instance, we can get the Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(tweets, labels)\n",
    "nb = model.named_steps['naive-bayes']\n",
    "feature_probabilities = nb.feature_log_prob_\n",
    "top_features = np.argsort(-feature_probabilities[1])[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this model, we can extract the probabilities for each word. These are stored as log probabilities, which is simply $log(P(A|f))$, where $f$ is a given feature.\n",
    "\n",
    "The preceding code will just give us the indices and not the actual feature values. This isn't very useful, so we will map the feature's indices to the actual values. The key is the `DictVectorizer` step of the pipeline, which created the matrices for us. Luckily this also records the mapping, allowing us to find the feature names that correlate to different columns.\n",
    "\n",
    "From here, we can print out the names of the top features by looking them up in the feature_names_ attribute of DictVectorizer. Enter the following lines into a new cell and run it to print out a list of the top features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.49056603773584906\n",
      "1 @ 0.471698113207547\n",
      "2 http 0.43396226415094336\n",
      "3 Python 0.32075471698113206\n",
      "4 - 0.169811320754717\n"
     ]
    }
   ],
   "source": [
    "dv = model.named_steps['vectorizer']\n",
    "for i, feature_index in enumerate(top_features):\n",
    "    if i < 20:\n",
    "        print(i, dv.feature_names_[feature_index],\n",
    "        np.exp(feature_probabilities[1][feature_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few features include :, http, # and @. These are likely to be noise (although the use of a colon is not very common outside programming), based on the data we collected. Collecting more data is critical to smoothing out these issues. Looking through the list though, we get a number of more obvious programming features:\n",
    "\n",
    "There are some others too that refer to Python in a work context, and therefore might be referring to the programming language (although freelance snake handlers may also use similar terms, they are less common on Twitter):\n",
    "\n",
    "Looking through these features gives us quite a few benefits. We could train people to recognize these tweets, look for commonalities (which give insight into a topic), or even get rid of features that make no sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
